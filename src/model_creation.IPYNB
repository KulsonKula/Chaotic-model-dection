{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data=pd.read_csv(data_path)\n",
    "    X = data.iloc[: ,:-1].to_numpy()\n",
    "    Y = data['labels'].to_numpy()\n",
    "\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def create_compile_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7174/7174 [==============================] - 244s 34ms/step - loss: 0.0404 - accuracy: 0.9895 - val_loss: 0.0080 - val_accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "   5/7174 [..............................] - ETA: 3:55 - loss: 0.0017 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7174/7174 [==============================] - 243s 34ms/step - loss: 0.0129 - accuracy: 0.9971 - val_loss: 0.0067 - val_accuracy: 0.9989 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "7174/7174 [==============================] - 195s 27ms/step - loss: 0.0112 - accuracy: 0.9978 - val_loss: 0.0369 - val_accuracy: 0.9921 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "7174/7174 [==============================] - 193s 27ms/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.0064 - val_accuracy: 0.9986 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "7174/7174 [==============================] - 192s 27ms/step - loss: 0.0089 - accuracy: 0.9984 - val_loss: 0.0042 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "7174/7174 [==============================] - 248s 35ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0043 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "7174/7174 [==============================] - 239s 33ms/step - loss: 0.0070 - accuracy: 0.9988 - val_loss: 0.0043 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "7174/7174 [==============================] - 200s 28ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.0016 - val_accuracy: 0.9998 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "7174/7174 [==============================] - 194s 27ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0016 - val_accuracy: 0.9997 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "7174/7174 [==============================] - 195s 27ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0064 - val_accuracy: 0.9989 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = load_data(\"../data/data_proccesed_50.csv\")\n",
    "\n",
    "def custom_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.02))\n",
    "scheduler = LearningRateScheduler(custom_scheduler)\n",
    "\n",
    "normalizer = layers.Normalization()\n",
    "normalizer.adapt(x_train)\n",
    "\n",
    "model_lstm_50 = models.Sequential([\n",
    "    layers.Input(shape=(x_train.shape[1], x_train.shape[2])), \n",
    "    normalizer,\n",
    "    layers.LSTM(64, activation=\"tanh\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=r\"../model/model_50.h5\",  \n",
    "    monitor='val_accuracy',        \n",
    "    verbose=1,                 \n",
    "    save_best_only=True,       \n",
    "    mode='max'                 \n",
    ")\n",
    "\n",
    "model_lstm_50.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_lstm_50=model_lstm_50.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), callbacks=[checkpoint, scheduler])\n",
    "\n",
    "# model_lstm_50.save(r\"../model/model_50.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3586/3587 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9435\n",
      "Epoch 1: val_accuracy improved from -inf to 0.99326, saving model to ../model\\model_100.h5\n",
      "3587/3587 [==============================] - 183s 51ms/step - loss: 0.1614 - accuracy: 0.9435 - val_loss: 0.0274 - val_accuracy: 0.9933 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "   3/3587 [..............................] - ETA: 2:38 - loss: 0.0187 - accuracy: 0.9948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9948\n",
      "Epoch 2: val_accuracy improved from 0.99326 to 0.99760, saving model to ../model\\model_100.h5\n",
      "3587/3587 [==============================] - 185s 52ms/step - loss: 0.0249 - accuracy: 0.9948 - val_loss: 0.0141 - val_accuracy: 0.9976 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3586/3587 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9961\n",
      "Epoch 3: val_accuracy did not improve from 0.99760\n",
      "3587/3587 [==============================] - 186s 52ms/step - loss: 0.0209 - accuracy: 0.9961 - val_loss: 0.0161 - val_accuracy: 0.9971 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9977\n",
      "Epoch 4: val_accuracy improved from 0.99760 to 0.99807, saving model to ../model\\model_100.h5\n",
      "3587/3587 [==============================] - 186s 52ms/step - loss: 0.0142 - accuracy: 0.9977 - val_loss: 0.0118 - val_accuracy: 0.9981 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9968\n",
      "Epoch 5: val_accuracy did not improve from 0.99807\n",
      "3587/3587 [==============================] - 219s 61ms/step - loss: 0.0174 - accuracy: 0.9968 - val_loss: 0.0135 - val_accuracy: 0.9978 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3586/3587 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9982\n",
      "Epoch 6: val_accuracy improved from 0.99807 to 0.99876, saving model to ../model\\model_100.h5\n",
      "3587/3587 [==============================] - 244s 68ms/step - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.0085 - val_accuracy: 0.9988 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9984\n",
      "Epoch 7: val_accuracy did not improve from 0.99876\n",
      "3587/3587 [==============================] - 201s 56ms/step - loss: 0.0100 - accuracy: 0.9984 - val_loss: 0.0098 - val_accuracy: 0.9979 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9984\n",
      "Epoch 8: val_accuracy improved from 0.99876 to 0.99888, saving model to ../model\\model_100.h5\n",
      "3587/3587 [==============================] - 204s 57ms/step - loss: 0.0100 - accuracy: 0.9984 - val_loss: 0.0075 - val_accuracy: 0.9989 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9988\n",
      "Epoch 9: val_accuracy did not improve from 0.99888\n",
      "3587/3587 [==============================] - 256s 71ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.0078 - val_accuracy: 0.9988 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9988\n",
      "Epoch 10: val_accuracy did not improve from 0.99888\n",
      "3587/3587 [==============================] - 278s 77ms/step - loss: 0.0080 - accuracy: 0.9988 - val_loss: 0.0078 - val_accuracy: 0.9988 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = load_data(\"../data/data_proccesed_100.csv\")\n",
    "\n",
    "model_lstm_100 = models.Sequential([\n",
    "    layers.Input(shape=(x_train.shape[1], x_train.shape[2])), \n",
    "    normalizer,\n",
    "    layers.LSTM(64, activation=\"tanh\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model_lstm_100.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=r\"../model/model_100.h5\",  \n",
    "    monitor='val_accuracy',        \n",
    "    verbose=1,                 \n",
    "    save_best_only=True,       \n",
    "    mode='max'                 \n",
    ")\n",
    "\n",
    "\n",
    "history_lstm_100=model_lstm_100.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), callbacks=[checkpoint,scheduler])\n",
    "# model_lstm_100.save(r\"../model/model_100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9194\n",
      "Epoch 1: val_accuracy improved from -inf to 0.92518, saving model to ../model\\model_200.h5\n",
      "1794/1794 [==============================] - 270s 148ms/step - loss: 0.2332 - accuracy: 0.9194 - val_loss: 0.4087 - val_accuracy: 0.9252 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "   1/1794 [..............................] - ETA: 4:07 - loss: 0.4509 - accuracy: 0.9219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1794/1794 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9608\n",
      "Epoch 2: val_accuracy improved from 0.92518 to 0.99042, saving model to ../model\\model_200.h5\n",
      "1794/1794 [==============================] - 249s 139ms/step - loss: 0.1556 - accuracy: 0.9608 - val_loss: 0.0585 - val_accuracy: 0.9904 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9820\n",
      "Epoch 3: val_accuracy did not improve from 0.99042\n",
      "1794/1794 [==============================] - 242s 135ms/step - loss: 0.0714 - accuracy: 0.9820 - val_loss: 0.0739 - val_accuracy: 0.9793 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9725\n",
      "Epoch 4: val_accuracy did not improve from 0.99042\n",
      "1794/1794 [==============================] - 276s 154ms/step - loss: 0.1065 - accuracy: 0.9725 - val_loss: 0.0877 - val_accuracy: 0.9769 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9618\n",
      "Epoch 5: val_accuracy did not improve from 0.99042\n",
      "1794/1794 [==============================] - 254s 142ms/step - loss: 0.1128 - accuracy: 0.9618 - val_loss: 0.1410 - val_accuracy: 0.9461 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9765\n",
      "Epoch 6: val_accuracy improved from 0.99042 to 0.99453, saving model to ../model\\model_200.h5\n",
      "1794/1794 [==============================] - 250s 140ms/step - loss: 0.0780 - accuracy: 0.9765 - val_loss: 0.0302 - val_accuracy: 0.9945 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9751\n",
      "Epoch 7: val_accuracy improved from 0.99453 to 0.99509, saving model to ../model\\model_200.h5\n",
      "1794/1794 [==============================] - 244s 136ms/step - loss: 0.0970 - accuracy: 0.9751 - val_loss: 0.0298 - val_accuracy: 0.9951 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9966\n",
      "Epoch 8: val_accuracy improved from 0.99509 to 0.99822, saving model to ../model\\model_200.h5\n",
      "1794/1794 [==============================] - 249s 139ms/step - loss: 0.0209 - accuracy: 0.9966 - val_loss: 0.0119 - val_accuracy: 0.9982 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9846\n",
      "Epoch 9: val_accuracy improved from 0.99822 to 0.99850, saving model to ../model\\model_200.h5\n",
      "1794/1794 [==============================] - 265s 148ms/step - loss: 0.0557 - accuracy: 0.9846 - val_loss: 0.0110 - val_accuracy: 0.9985 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9978\n",
      "Epoch 10: val_accuracy did not improve from 0.99850\n",
      "1794/1794 [==============================] - 247s 138ms/step - loss: 0.0140 - accuracy: 0.9978 - val_loss: 0.0172 - val_accuracy: 0.9968 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = load_data(\"../data/data_proccesed_200.csv\")\n",
    "\n",
    "model_lstm_200 = models.Sequential([\n",
    "    layers.Input(shape=(x_train.shape[1], x_train.shape[2])), \n",
    "    normalizer,\n",
    "    layers.LSTM(64, activation=\"tanh\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model_lstm_200.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=r\"../model/model_200.h5\",  \n",
    "    monitor='val_accuracy',        \n",
    "    verbose=1,                 \n",
    "    save_best_only=True,       \n",
    "    mode='max'                 \n",
    ")\n",
    "\n",
    "history_lstm_200=model_lstm_200.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), callbacks=[checkpoint,scheduler])\n",
    "\n",
    "# model_lstm_200.save(r\"../model/model_200.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_scheduler(epoch, lr):\n",
    "#     if epoch < 10:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         return float(lr * tf.math.exp(-0.02))\n",
    "# scheduler = LearningRateScheduler(custom_scheduler)\n",
    "\n",
    "# normalizer = layers.Normalization()\n",
    "# normalizer.adapt(x_train)\n",
    "\n",
    "# model_cnn = models.Sequential([\n",
    "#     layers.Input(shape=(x_train.shape[1], x_train.shape[2])), \n",
    "#     normalizer,\n",
    "#     layers.Conv1D(64, 1, activation=\"relu\"),\n",
    "#     # layers.MaxPooling1D(pool_size=2),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(1, activation=\"sigmoid\"),\n",
    "# ])\n",
    "\n",
    "# model_cnn.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(),\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy'],\n",
    "# )\n",
    "\n",
    "# history_cnn=model_cnn.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), callbacks=[scheduler])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
